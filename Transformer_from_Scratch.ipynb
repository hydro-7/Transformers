{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fb5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63c6dc",
   "metadata": {},
   "source": [
    "## **Image Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c74bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, image_height, image_width, image_channels, patch_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Image : c,w,h : 3, 224, 224   -->   Patch : n * c,p,p : 3, 16, 16    -->   Embedding : embedding_dim\n",
    "        \n",
    "        self.patch_height, self.patch_width = patch_size, patch_size\n",
    "        num_patches = (image_height // self.patch_height) * (image_width // self.patch_width)\n",
    "\n",
    "        patch_dim = image_channels * self.patch_height * self.patch_width                           # cp^2\n",
    "\n",
    "        \n",
    "        self.patch_to_embed = nn.Linear(patch_dim, embedding_dim)                                   # Linear layer that converts the patch vectors to embeddings\n",
    "\n",
    "        # nn.Parameter => trainable params\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embedding_dim))           # num_patches + 1  -->  every patch will get an embedding + 1 CLS token   && each embedding will be of len 'embedding_dim'\n",
    "        self.cls_token = nn.Parameter(torch.randn(embedding_dim))\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # input is of the shape [ b, c, h, w ]\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        # [ b, c, h, w ] --> [ b, num_patches, patch_dim ]   :   [ b, h*w/p*p, cp^2 ]\n",
    "\n",
    "        output = rearrange(X, 'b c (nh ph) (nw pw) -> b (nh nw) (ph pw c)',  ph = self.patch_height, pw = self.patch_width)                           \n",
    "        # even though nh & nw arent explicitly mentioned, it is understood from the shape of X\n",
    "                           \n",
    "        \n",
    "        output = self.patch_to_embed(output)  \n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, 'd -> b 1 d', b = batch_size)                           # Repeat for every image in the batch size  \n",
    "        output = torch.cat((cls_tokens, output), dim = 1)                                           # add the class token\n",
    "\n",
    "        output += self.pos_embedding\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13c1cd",
   "metadata": {},
   "source": [
    "## **Attention Mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6a59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input is given as n embeddings of length p^2c => [ num_patches, D ]\n",
    "        # This input is transformed using a weight matrix W to get Q, K, V :    \n",
    "\n",
    "        # [ num_patches, D ]  x  [ D, nh * 3 * hd ]  -->  [ num_patches, nh * 3 * hd]\n",
    "        \n",
    "        self.num_heads = num_heads                                      # nh\n",
    "        self.head_dim = head_dim                                        # hd\n",
    "        self.embedding_dim = embedding_dim                              # D\n",
    "\n",
    "        self.attention_dim = self.num_heads * self.head_dim             # nh * hd\n",
    "\n",
    "        # This is the layer that acts as Wq Wk Wv for the embeddings and converts them to Q K V\n",
    "        self.wq_wk_wv_projection = nn.Linear(self.embedding_dim, self.attention_dim * 3, bias= False)\n",
    "\n",
    "        self.output_proj = nn.Linear(self.attention_dim, self. embedding_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Input X --> [ batch, num_patches, D ]\n",
    "        B, N = X.shape[:2]\n",
    "\n",
    "        # Projecting this input X to Q,K,V : [ B, N, D ] --> [ B, N, nh * 3 * hd ]\n",
    "        # Splitting into 3 parts [ B, N, nh * 3 * hd ] --> 3 x [ B, N, nh * hd ] \n",
    "\n",
    "        Q, K, V = self.wq_wk_wv_projection(X).split(self.attention_dim, dim = -1) \n",
    "\n",
    "        # Rearranging [ B, N, nh * hd ] --> [ B, nh, N, hd ]\n",
    "        Q = rearrange(Q, 'b n (nh hd) -> b nh n hd', nh = self.num_heads, hd = self.head_dim)\n",
    "        K = rearrange(K, 'b n (nh hd) -> b nh n hd', nh = self.num_heads, hd = self.head_dim)\n",
    "        V = rearrange(V, 'b n (nh hd) -> b nh n hd', nh = self.num_heads, hd = self.head_dim)\n",
    "\n",
    "        # att = Q x K.T / sqrt(hd)\n",
    "        # att = softmax(att)\n",
    "        # output = att x V\n",
    "        # concat all the heads \n",
    "\n",
    "        att = torch.matmul(Q, K.transpose(-2, -1)) * (self.head_dim ** (-0.5))\n",
    "        att = torch.nn.functional.softmax(att, dim = -1)\n",
    "        \n",
    "        output = torch.matmul(att, V)\n",
    "        output = rearrange(output, 'b nh n hd -> b n (nh hd)', nh = self.num_heads, hd = self.head_dim)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa94cf1",
   "metadata": {},
   "source": [
    "## **Transformer Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02d22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, num_heads, head_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ff_embedding_dim = embedding_dim * 4    # Usually a large value like 2048\n",
    "        \n",
    "        self.normalize = nn.LayerNorm(embedding_dim)\n",
    "        self.attention_block = Attention(num_heads, head_dim, embedding_dim)\n",
    "        self.feed_forward_block = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_embedding_dim),\n",
    "            nn.GeLU(),\n",
    "            nn.Linear(ff_embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out = X\n",
    "        out = out + self.attention_block(self.normalize(out))\n",
    "        out = out + self.feed_forward_block(self.normalize(out))\n",
    "        return out       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f8516d",
   "metadata": {},
   "source": [
    "## **Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd37fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, num_classes, image_height, image_width, image_channels, patch_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding Layer responsible for patchification\n",
    "        self.patch_embedding_layer = Embedding(image_height, image_width, image_channels, patch_size, embedding_dim)\n",
    "\n",
    "        # The several transformer layers, stacked together\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerLayer() for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Normalization block\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Final layer, this is responsible for making the prediction as it assigns probabilities to each class using the CLS token\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        out = self.patch_embedding_layer(X)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.norm(out)\n",
    "\n",
    "        out = self.fc(out[:, 0])        # calculates for the CLS token\n",
    "        return out       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
